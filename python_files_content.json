{
    "ex_code_commented.py": "# Recursively sorts an array using the merge sort algorithm.\ndef merge_sort(arr):\n    if len(arr) > 1:\n        mid = len(arr) // 2  # Find the middle of the array\n        left_half = arr[:mid]  # Dividing the elements into 2 halves\n        right_half = arr[mid:]\n\n        merge_sort(left_half)  # Sorting the first half\n        merge_sort(right_half)  # Sorting the second half\n\n        i = j = k = 0\n\n        # Copy data to temp arrays L[] and R[]\n        while i < len(left_half) and j < len(right_half):\n            if left_half[i] < right_half[j]:\n                arr[k] = left_half[i]\n                i += 1\n            else:\n                arr[k] = right_half[j]\n                j += 1\n            k += 1\n\n        # Checking if any element was left\n        while i < len(left_half):\n            arr[k] = left_half[i]\n            i += 1\n            k += 1\n\n        while j < len(right_half):\n            arr[k] = right_half[j]\n            j += 1\n            k += 1\n\n    return arr\n\n# deez monumental nuts\n\ndef read_and_sort(input_file, output_file):\n    with open(input_file, 'r') as file:\n        numbers = file.read().splitlines()\n\n    # Convert strings to integers\n    numbers = [int(num) for num in numbers]\n\n    # Sort the numbers using merge sort\n    sorted_numbers = merge_sort(numbers)\n\n    # Write the sorted numbers to an output file\n    with open(output_file, 'w') as file:\n        for num in sorted_numbers:\n            file.write(f\"{num}\\n\")\n\n    print(f\"Sorted numbers have been written to {output_file}\")\n\n\nif __name__ == \"__main__\":\n    input_file = \"input_numbers.txt\"\n    output_file = \"sorted_numbers.txt\"\n    read_and_sort(input_file, output_file)\n",
    "calibrate_thresh.py": "import json\nfrom math import ceil\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport os\nfrom scipy.stats import rankdata\nfrom sayless import (\n    get_frequency_scores,\n    get_subclaims,\n    merge_subclaims,\n    query_model,\n    default_merge_prompt,\n)\nimport random\n\nCORRECT_ANNOTATIONS = [\"Y\", \"S\"]\n\n\n# Returns the legend name based on the provided confidence method.\ndef get_legend_name(confidence_method):\n    if (\n        confidence_method == \"frequency+gpt\"\n        or confidence_method == \"frequency+gpt-ranking\"\n    ):\n        return \"Frequency\"\n    elif confidence_method == \"baseline\" or confidence_method == \"baseline-ranking\":\n        return \"Ordinal\"\n    elif confidence_method == \"gpt\" or confidence_method == \"gpt-ranking\":\n        return \"GPT-4 confidence\"\n    elif confidence_method == \"random\" or confidence_method == \"random-ranking\":\n        return \"Random\"\n    elif confidence_method == \"optimal\" or confidence_method == \"optimal-ranking\":\n        return \"Optimal\"\n\n\n# Returns a formatted title name based on the given dataset prefix.\ndef get_title_name(dataset_prefix):\n    if dataset_prefix == \"factscore\":\n        return \"FActScore\"\n    elif dataset_prefix == \"nq\":\n        return \"NQ\"\n    elif dataset_prefix == \"MATH\":\n        return \"MATH\"\n    return dataset_prefix\n\n\n# Writes the output_list to a JSON Lines file specified by filename.\ndef dump_claims(output_list, filename=\"claims.jsonl\"):\n    \"\"\"\n    Dumps output_list into filename.\n    [{\"prompt\": \"Who is Tatsu?\", \"claims\": [{\"subclaim\": \"Tatsu is Japanese person\", 'correct': 1.0}, {\"subclaim\": \"Tatsu was born in 1988\", 'correct': 0.0} ..]}]\n    \"\"\"\n    with open(filename, \"w\") as outfile:\n        merged_json = {\"data\": output_list}\n        json.dump(merged_json, outfile, indent=4)\n\n\n# Loads calibration data from a specified JSON Lines file.\ndef load_calibration(filename=\"claims.jsonl\"):\n    \"\"\"\n    Reverse of dump_claims.\n    \"\"\"\n    with open(filename, \"r\") as fopen:\n        return json.load(fopen)[\"data\"]\n\n\n# Returns ranking scores based on confidence method and optional percentage normalization.\ndef get_ranking(entry, confidence_method, use_percent=True):\n    \"\"\"\n    Returns the corresponding ranking scores from the raw scores of confidence_method.\n    \"\"\"\n    score_list = [\n        -(subclaim[confidence_method + \"-score\"] + subclaim[\"noise\"])\n        for subclaim in entry[\"claims\"]\n    ]\n    rankings = len(entry[\"claims\"]) + 1 - rankdata(score_list, method=\"ordinal\")\n    if use_percent:\n        rankings = rankings / len(entry[\"claims\"])\n    return rankings\n\n\n# Returns a list of confidence scores for an entry using the specified method.\ndef get_confidence(entry, method, openai_client, model):\n    \"\"\"\n    Takes in an entry from {}_annotations.jsonl and returns a list of confidence scores from method.\n    \"\"\"\n    if method == \"random\":\n        return [np.random.normal(0, 1) for subclaim in entry[\"claims\"]]\n    elif method == \"baseline\":\n        return [\n            len(entry[\"claims\"]) - x for x in list(range(1, len(entry[\"claims\"]) + 1))\n        ]\n    elif method == \"gpt\":\n        return [float(subclaim[\"gpt-score\"]) for subclaim in entry[\"claims\"]]\n    elif method == \"frequency\":\n        return get_frequency_scores(\n            openai_client, entry[\"claims\"], entry[\"prompt\"], 5, model\n        )\n    # This assumes frequency was already added.\n    elif method == \"frequency+gpt\":\n        return [\n            subclaim[\"gpt-score\"] + subclaim[\"frequency-score\"]\n            for subclaim in entry[\"claims\"]\n        ]\n    elif method == \"optimal\":\n        return [\n            int(subclaim[\"annotation\"] in CORRECT_ANNOTATIONS)\n            for subclaim in entry[\"claims\"]\n        ]\n    # This assumes the corresponding raw scores were already added.\n    elif method in [\n        \"random-ranking\",\n        \"baseline-ranking\",\n        \"gpt-ranking\",\n        \"frequency-ranking\",\n        \"frequency+gpt-ranking\",\n        \"optimal-ranking\",\n    ]:\n        return get_ranking(\n            entry, method[:-8]\n        )  # -8 is to remove '-ranking' from method.\n    else:\n        print(f\"{method} method is not implemented.\")\n\n\n# Adds noise to calibration data and computes confidence scores for specified methods, saving results to a file.\ndef add_scores(calibration_data, filename, confidence_methods, openai_client, model):\n    \"\"\"\n    Adds noise (to break ties later) and scores for each method in confidence_methods to filename.\n    \"\"\"\n    # Add a random draw to the data if it does not already exist\n    if \"noise\" not in calibration_data[0][\"claims\"][0]:\n        for entry in tqdm(calibration_data):\n            for i, output in enumerate(entry[\"claims\"]):\n                output[\"noise\"] = np.random.normal(0, 0.001)\n\n        # Write to file if any modification was made.\n        dump_claims(calibration_data, filename)\n\n    # If confidence_method is not already computed, compute and add it to calibration_data.\n    for confidence_method in confidence_methods:\n        if confidence_method + \"-score\" not in calibration_data[0][\"claims\"][0]:\n            print(f\"Computing {confidence_method} method\")\n            for entry in tqdm(calibration_data):\n                score_list = get_confidence(\n                    entry, confidence_method, openai_client, model\n                )\n                for i, output in enumerate(entry[\"claims\"]):\n                    output[confidence_method + \"-score\"] = score_list[i]\n\n        # Write to file if any modification was made.\n        dump_claims(calibration_data, filename)\n\n    return calibration_data\n\n\n# Calculate the r_a score for an entry using a specified confidence method and threshold a.\ndef get_r_score(entry, confidence_method, a):\n    \"\"\"\n    Compute the r_a score for entry when confidence_method is used as the sub-claim scoring function.\n    \"\"\"\n    threshold_set = sorted(\n        [\n            subclaim[confidence_method + \"-score\"] + subclaim[\"noise\"]\n            for subclaim in entry[\"claims\"]\n        ],\n        reverse=True,\n    )\n    curr_threshold = threshold_set[0]\n    for threshold in threshold_set:\n        curr_threshold = threshold\n        # Apply threshold.\n        accepted_subclaims = [\n            subclaim\n            for subclaim in entry[\"claims\"]\n            if subclaim[confidence_method + \"-score\"] + subclaim[\"noise\"] >= threshold\n        ]\n\n        # Compute entailed/correct fraction.\n        entailed_fraction = (\n            np.mean(\n                [\n                    subclaim[\"annotation\"] in CORRECT_ANNOTATIONS\n                    for subclaim in accepted_subclaims\n                ]\n            )\n            if accepted_subclaims\n            else 1\n        )\n\n        if entailed_fraction < a:\n            return curr_threshold\n    return -100000  # -100000 is less than any score assigned by any of the implemented confidence methods\n\n\n# Computes the threshold for conformal prediction based on given alpha, calibration data, and confidence method.\ndef compute_threshold(alpha, calibration_data, a, confidence_method):\n    \"\"\"\n    Computes the quantile/threshold from conformal prediction.\n    # alpha: float in (0, 1)\n    # calibration_data: calibration data\n    # a: as in paper, required fraction correct\n    # confidence_method: string\n    \"\"\"\n    # Compute r score for each example.\n    r_scores = [get_r_score(entry, confidence_method, a) for entry in calibration_data]\n\n    # Compute threshold for conformal prection. The quantile is ceil((n+1)*(1-alpha))/n, and\n    # We map this to the index by dropping the division by n and subtracting one (for zero-index).\n    quantile_target_index = ceil((len(r_scores) + 1) * (1 - alpha))\n    threshold = sorted(r_scores)[quantile_target_index - 1]\n    return threshold\n\n\n# Generates a leave-one-out conformal plot showing the relationship between correctness and fraction of removed claims for various confidence methods.\ndef create_correctness_vs_removed_plot(\n    dataset_prefix, data, alphas, a, confidence_methods, fig_filename\n):\n    \"\"\"\n    Creates leave-one-out conformal plots.\n    \"\"\"\n    print(f\"Producing conformal plot: {fig_filename}\")\n    plt.figure(dpi=800)\n\n    for confidence_method in tqdm(confidence_methods):\n        results = []  # first indexes into alpha, then list of (correct, frac_removed)_i\n\n        for alpha in alphas:\n            results_for_alpha = [[], []]\n            for i in range(len(data)):\n                calibration_data = data[:i] + data[i + 1 :]\n                test_data = data[i]\n\n                threshold = compute_threshold(\n                    alpha, calibration_data, a, confidence_method\n                )\n                accepted_subclaims = [\n                    subclaim\n                    for subclaim in test_data[\"claims\"]\n                    if subclaim[confidence_method + \"-score\"] + subclaim[\"noise\"]\n                    >= threshold\n                ]\n                fraction_removed = 1 - len(accepted_subclaims) / len(\n                    test_data[\"claims\"]\n                )\n                entailed_fraction = (\n                    np.mean(\n                        [\n                            subclaim[\"annotation\"] in CORRECT_ANNOTATIONS\n                            for subclaim in accepted_subclaims\n                        ]\n                    )\n                    if accepted_subclaims\n                    else 1\n                )\n                correctness = entailed_fraction >= a\n                results_for_alpha[0].append(correctness)\n                results_for_alpha[1].append(fraction_removed)\n\n            results.append(results_for_alpha)\n\n        x = [np.mean(results_for_alpha[0]) for results_for_alpha in results]\n        y = [np.mean(results_for_alpha[1]) for results_for_alpha in results]\n\n        # Add standard error.\n        yerr = [\n            np.std(results_for_alpha[1]) * 1.96 / np.sqrt(len(results_for_alpha[1]))\n            for results_for_alpha in results\n        ]\n        label = get_legend_name(confidence_method)\n\n        plt.errorbar(x, y, yerr=yerr, label=label, linewidth=2)\n\n    # Plot base factuality point.\n    x_point = x[-1]\n    y_point = y[-1]\n    point_size = 235\n    plt.scatter(\n        x_point,\n        y_point,\n        color=\"black\",\n        marker=\"*\",\n        s=point_size,\n        label=\"Base factuality\",\n        zorder=1000,\n    )\n\n    font_size = 16\n    legend_font_size = 13\n    dataset_title_name = get_title_name(dataset_prefix)\n    if a == 1:\n        plt.title(dataset_title_name, fontsize=font_size + 4)\n        plt.xlabel(\"Fraction of factual outputs\", fontsize=font_size)\n    else:\n        plt.title(f\"{dataset_title_name}, a={a}\", fontsize=font_size + 4)\n        plt.xlabel(f\"Fraction achieving avg factuality >= {a}\")\n    plt.ylabel(\"Average percent removed\", fontsize=font_size)\n\n    legend = plt.legend(\n        loc=\"upper left\", bbox_to_anchor=(1, 1), fontsize=legend_font_size\n    )\n    legend.get_title().set_fontsize(legend_font_size)\n    plt.savefig(fig_filename, bbox_inches=\"tight\")\n\n\n# Generates a calibration plot comparing target factuality to empirical factuality using shuffled data and confidence thresholds.\ndef create_calibration_plot(\n    dataset_prefix, data, alphas, a, confidence_method, fig_filename\n):\n    \"\"\"\n    Creates calibration plot.\n    \"\"\"\n    print(f\"Producing calibration plot: {fig_filename}\")\n    fig, ax = plt.subplots(figsize=(6, 4))\n\n    results = []  # first indexes into alpha. then list of (correct, frac_removed)_i\n\n    for alpha in tqdm(alphas):\n        results_for_alpha = [[], []]\n        for i in range(1000):\n            # Randomly shuffle the data\n            random.shuffle(data)\n\n            # Split the data into two equal parts\n            split_index = len(data) // 2\n            calibration_data = data[:split_index]\n            test_data = data[split_index:]\n\n            threshold = compute_threshold(alpha, calibration_data, a, confidence_method)\n\n            accepted_subclaim_list = [\n                [\n                    subclaim\n                    for subclaim in test_data_point[\"claims\"]\n                    if subclaim[confidence_method + \"-score\"] + subclaim[\"noise\"]\n                    >= threshold\n                ]\n                for test_data_point in test_data\n            ]\n            entailed_fraction_list = [\n                (\n                    np.mean(\n                        [\n                            subclaim[\"annotation\"] in CORRECT_ANNOTATIONS\n                            for subclaim in accepted_subclaims\n                        ]\n                    )\n                    if accepted_subclaims\n                    else 1\n                )\n                for accepted_subclaims in accepted_subclaim_list\n            ]\n            correctness_list = [\n                entailed_fraction >= a for entailed_fraction in entailed_fraction_list\n            ]\n            fraction_correct = sum(correctness_list) / len(correctness_list)\n            results_for_alpha[0].append(1 - alpha)\n            results_for_alpha[1].append(fraction_correct)\n\n        results.append(results_for_alpha)\n\n    x = [np.mean(results_for_alpha[0]) for results_for_alpha in results]\n    y = [np.mean(results_for_alpha[1]) for results_for_alpha in results]\n    # yerr = [np.std(results_for_alpha[1]) for results_for_alpha in results]\n\n    print(x)\n    print(y)\n    # plt.fill_between(np.array(x), np.array(y) - np.array(yerr), np.array(y) + np.array(yerr), color=\"#ADD8E6\")\n\n    x_values = np.linspace(0.3, 0.98, 100)\n\n    # Plot lower bound.\n    y_values = x_values\n    plt.plot(\n        x_values, y_values, \"--\", color=\"gray\", linewidth=2, label=\"Thrm 3.1 bounds\"\n    )\n\n    # Plot upper bound\n    y_values = x_values + 1 / (split_index + 1)\n    plt.plot(x_values, y_values, \"--\", color=\"gray\", linewidth=2)\n    plt.plot(x, y, label=get_title_name(dataset_prefix), linewidth=2)\n\n    plt.xlabel(f\"Target factuality (1 - {chr(945)})\", fontsize=16)\n    plt.legend()\n    plt.ylabel(\"Empirical factuality\", fontsize=16)\n    plt.savefig(fig_filename, bbox_inches=\"tight\", dpi=800)\n\n\n# Generates a JSONL file with original outputs and newly accepted subclaims based on a confidence threshold.\ndef generate_merged_outputs(\n    data,\n    alpha,\n    a,\n    confidence_method,\n    openai_client,\n    model,\n    merged_filename,\n    merge_prompt,\n):\n    \"\"\"\n    Creates jsonl file with original output and new suclaims.\n    \"\"\"\n    print(\n        f\"Merging accepted subclaims for a={a}, alpha={alpha} and confidence_method={confidence_method}\"\n    )\n    for i in tqdm(range(len(data))):\n        calibration_data = data[:i] + data[i + 1 :]\n        test_data = data[i]\n\n        threshold = compute_threshold(alpha, calibration_data, a, confidence_method)\n        accepted_subclaims = [\n            subclaim\n            for subclaim in test_data[\"claims\"]\n            if subclaim[confidence_method + \"-score\"] + subclaim[\"noise\"] >= threshold\n        ]\n        test_data[\"new-output\"] = merge_subclaims(\n            openai_client,\n            accepted_subclaims,\n            model,\n            test_data[\"prompt\"],\n            create_merge_prompt=merge_prompt,\n        )\n        test_data[\"all-subclaims\"] = [\n            {\"subclaim\": subclaim[\"subclaim\"], \"annotation\": subclaim[\"annotation\"]}\n            for subclaim in test_data[\"claims\"]\n        ]\n        test_data[\"accepted-subclaims\"] = [\n            {\"subclaim\": subclaim[\"subclaim\"], \"annotation\": subclaim[\"annotation\"]}\n            for subclaim in accepted_subclaims\n        ]\n\n    merged_data = (\n        [\n            {\n                \"prompt\": entry[\"prompt\"],\n                \"original-output\": entry[\"original-output\"],\n                \"all-subclaims\": entry[\"all-subclaims\"],\n                \"accepted-subclaims\": entry[\"accepted-subclaims\"],\n                \"new-output\": entry[\"new-output\"],\n            }\n            for entry in data\n        ]\n        if \"original-output\" in calibration_data[0]\n        else [\n            {\n                \"prompt\": entry[\"prompt\"],\n                \"all-subclaims\": entry[\"all-subclaims\"],\n                \"accepted-subclaims\": entry[\"accepted-subclaims\"],\n                \"new-output\": entry[\"new-output\"],\n            }\n            for entry in data\n        ]\n    )\n\n    dump_claims(merged_data, merged_filename)\n\n\n# Generates a histogram of the fraction of subclaims removed across all outputs based on specified parameters.\ndef create_hist(\n    dataset_prefix,\n    data,\n    alpha,\n    a,\n    confidence_method,\n    openai_client,\n    model,\n    hist_filename,\n    merge_prompt,\n):\n    \"\"\"\n    Creates histogram showing the fraction of subclaims removed across all outputs.\n    \"\"\"\n    print(\n        f\"Creating histogram for a={a}, alpha={alpha} and confidence_method={confidence_method}\"\n    )\n    plt.figure(dpi=800)\n    fraction_removed_list = []\n    for i in tqdm(range(len(data))):\n        calibration_data = data[:i] + data[i + 1 :]\n        test_data = data[i]\n        threshold = compute_threshold(alpha, calibration_data, a, confidence_method)\n\n        accepted_subclaims = [\n            subclaim\n            for subclaim in test_data[\"claims\"]\n            if subclaim[confidence_method + \"-score\"] + subclaim[\"noise\"] >= threshold\n        ]\n        fraction_removed = 1 - len(accepted_subclaims) / len(test_data[\"claims\"])\n        fraction_removed_list.append(fraction_removed)\n\n    fontsize = 15\n    fig, ax = plt.subplots(figsize=(6, 3.5))\n    plt.xlabel(\"Percent removed\", fontsize=fontsize)\n    plt.ylabel(\"Fraction of outputs\", fontsize=fontsize)\n    plt.title(\n        f\"{get_title_name(dataset_prefix)}, {chr(945)}={alpha}\", fontsize=fontsize\n    )\n    weights = np.ones_like(fraction_removed_list) / float(len(fraction_removed_list))\n    plt.hist(fraction_removed_list, weights=weights)\n    plt.savefig(hist_filename, bbox_inches=\"tight\", dpi=800)\n\n\n# Analyzes a dataset by generating outputs, subclaims, and optionally creating plots and histograms based on various confidence methods and parameters.\ndef analyze_dataset(\n    dataset_prefix,\n    input_dataset,\n    openai_client,\n    model,\n    breakdown_prompt,\n    confidence_method,\n    confidence_methods_raw,\n    confidence_methods_ranking,\n    alpha,\n    alphas,\n    a,\n    compute_single_threshold=True,\n    merge=False,\n    create_plots=True,\n    create_histogram=False,\n    calib=False,\n    merge_prompt=default_merge_prompt,\n):\n    \"\"\"\n    Performs the desired analysis for a given dataset.\n    \"\"\"\n    # Generate outputs and subclaims if they do not exist, in {dataset_prefix}_subclaims.jsonl file that\n    # can be (manually) copied over to /data/{dataset_prefix}_annotations.jsonl and then annotated.\n    if not os.path.exists(f\"data/{dataset_prefix}_annotations.jsonl\"):\n        print(\n            f\"Creating dataset for annotation. When done, please copy out/{dataset_prefix}_subclaims.jsonl to data/{dataset_prefix}_annotations.jsonl and annotate.\"\n        )\n        data = []\n        # Generate outputs for each prompt\n        for prompt in tqdm(input_dataset):\n            output = query_model(openai_client, prompt, model)\n\n            # Extract subclaims. \"annotation\" field is automtically set to 'N'.\n            subclaims = get_subclaims(\n                openai_client, output, model, breakdown_prompt=breakdown_prompt\n            )\n            claim_list = [\n                {\n                    \"subclaim\": subclaim[\"subclaim\"],\n                    \"gpt-score\": subclaim[\"gpt-score\"],\n                    \"annotation\": \"N\",\n                }\n                for subclaim in subclaims\n            ]\n            data.append(\n                {\"prompt\": prompt, \"original-output\": output, \"claims\": claim_list}\n            )\n        dump_claims(data, f\"out/{dataset_prefix}_subclaims.jsonl\")\n\n    else:\n        # Otherwise, get the annotated subclaims add all scores if they are not already there.\n        if not os.path.exists(f\"out/{dataset_prefix}_subclaims_with_scores.jsonl\"):\n            print(\n                f\"Computing scores for subclaims. These will appear in out/{dataset_prefix}_subclaims_with_scores.jsonl\"\n            )\n            calibration_data = load_calibration(\n                f\"data/{dataset_prefix}_annotations.jsonl\"\n            )\n            add_scores(\n                calibration_data,\n                f\"out/{dataset_prefix}_subclaims_with_scores.jsonl\",\n                confidence_methods_raw + confidence_methods_ranking,\n                openai_client,\n                model,\n            )\n\n        calibration_data = load_calibration(\n            f\"out/{dataset_prefix}_subclaims_with_scores.jsonl\"\n        )\n\n        if compute_single_threshold:\n            # Compute a single threshold for ALPHA and CONFIDENCE_METHOD.\n            threshold = compute_threshold(\n                alpha,\n                calibration_data,\n                a,\n                confidence_method,\n            )\n            with open(\n                f\"out/{dataset_prefix}_a={a}_alpha={alpha}_conf={confidence_method}.txt\",\n                \"w\",\n            ) as fopen:\n                fopen.write(str(threshold))\n\n        if create_histogram:\n            create_hist(\n                dataset_prefix,\n                calibration_data,\n                alpha,\n                a,\n                confidence_method,\n                openai_client,\n                model,\n                f\"out/{dataset_prefix}_hist_a={a}_alpha={alpha}_conf={confidence_method}.png\",\n                merge_prompt,\n            )\n\n        if merge:\n            generate_merged_outputs(\n                calibration_data,\n                alpha,\n                a,\n                confidence_method,\n                openai_client,\n                model,\n                f\"out/{dataset_prefix}_merged_a={a}_alpha={alpha}_conf={confidence_method}.jsonl\",\n                merge_prompt,\n            )\n\n        if create_plots:\n            # Create plots for ALPHAS, CONFIDENCE_METHODS_RAW, CONFIDENCE_METHODS_RANKING.\n            create_correctness_vs_removed_plot(\n                dataset_prefix,\n                calibration_data,\n                alphas,\n                a,\n                confidence_methods_raw,\n                f\"out/{dataset_prefix}_raw_a={a}_fig.png\",\n            )\n            create_correctness_vs_removed_plot(\n                dataset_prefix,\n                calibration_data,\n                alphas,\n                a,\n                confidence_methods_ranking,\n                f\"out/{dataset_prefix}_ranking_a={a}_fig.png\",\n            )\n\n        if calib:\n            # Create calibration plots for ALPHAS, CONFIDENCE_METHODS_RAW, CONFIDENCE_METHODS_RANKING.\n            create_calibration_plot(\n                dataset_prefix,\n                calibration_data,\n                alphas,\n                a,\n                confidence_method,\n                f\"out/{dataset_prefix}_raw_calibration_a={a}.png\",\n            )\n",
    "ex_code.py": "def merge_sort(arr):\n    if len(arr) > 1:\n        mid = len(arr) // 2  # Find the middle of the array\n        left_half = arr[:mid]  # Dividing the elements into 2 halves\n        right_half = arr[mid:]\n\n        merge_sort(left_half)  # Sorting the first half\n        merge_sort(right_half)  # Sorting the second half\n\n        i = j = k = 0\n\n        # Copy data to temp arrays L[] and R[]\n        while i < len(left_half) and j < len(right_half):\n            if left_half[i] < right_half[j]:\n                arr[k] = left_half[i]\n                i += 1\n            else:\n                arr[k] = right_half[j]\n                j += 1\n            k += 1\n\n        # Checking if any element was left\n        while i < len(left_half):\n            arr[k] = left_half[i]\n            i += 1\n            k += 1\n\n        while j < len(right_half):\n            arr[k] = right_half[j]\n            j += 1\n            k += 1\n\n    return arr\n\n# deez monumental nuts\n\ndef read_and_sort(input_file, output_file):\n    with open(input_file, 'r') as file:\n        numbers = file.read().splitlines()\n\n    # Convert strings to integers\n    numbers = [int(num) for num in numbers]\n\n    # Sort the numbers using merge sort\n    sorted_numbers = merge_sort(numbers)\n\n    # Write the sorted numbers to an output file\n    with open(output_file, 'w') as file:\n        for num in sorted_numbers:\n            file.write(f\"{num}\\n\")\n\n    print(f\"Sorted numbers have been written to {output_file}\")\n\n\nif __name__ == \"__main__\":\n    input_file = \"input_numbers.txt\"\n    output_file = \"sorted_numbers.txt\"\n    read_and_sort(input_file, output_file)\n",
    "process_file.py": "from helpers.query import query_model\nimport ast\nimport re\n\ndef read_python_file(file_path: str) -> str:  \n    \"\"\"  \n    Reads the content of a Python file and returns it as a string.  \n      \n    :param file_path: The path to the Python file.  \n    :return: The content of the file as a string.  \n    \"\"\"  \n    try:  \n        with open(file_path, 'r', encoding='utf-8') as file:  \n            file_content = file.read()  \n    except FileNotFoundError:  \n        print(f\"The file {file_path} does not exist.\")  \n        return \"\"  \n    except Exception as e:  \n        print(f\"An error occurred while reading the file: {e}\")  \n        return \"\"  \n      \n    return file_content \n\ndef summarize_file(input_text: str):\n    \"\"\"  \n    Summarize the content of a file using OpenAI's GPT.  \n  \n    :param file_content: The content of the file as a string.  \n    :return: A summary of the file content.  \n    \"\"\"  \n    # Define the prompt for GPT  \n    summary_prompt = f\"Summarize the functionality of the following Python file:\\n\\n{input_text}\"\n\n    return query_model(prompt=summary_prompt)\n\ndef find_functions_with_no_comments(filename):\n    \"\"\"Parse a Python file to find functions without descriptive comments.\"\"\"\n    with open(filename, \"r\") as file:\n        tree = ast.parse(file.read())\n\n    functions = []\n    with open(filename, \"r\") as file:\n        lines = file.readlines()\n\n    for node in ast.walk(tree):\n        if isinstance(node, ast.FunctionDef):\n            function_start = node.lineno - 1\n\n            # Check lines above the function definition\n            has_comment = False\n            for i in range(function_start - 1, -1, -1):\n                line = lines[i].strip()\n\n                if line == \"\":\n                    continue\n                \n                # Check for single-line comment\n                if line.startswith(\"#\"):\n                    has_comment = True\n                # Check for multi-line triple-quote comment\n                elif line.startswith('\"\"\"') or line.startswith(\"'''\"):\n                    # If the triple quote starts and ends on the same line\n                    if line.endswith('\"\"\"') or line.endswith(\"'''\") and len(line) > 3:\n                        has_comment = True\n                    else:\n                        # Handle multi-line comments that start with triple quotes\n                        has_comment = True\n                        for j in range(i - 1, -1, -1):\n                            if lines[j].strip().endswith('\"\"\"') or lines[j].strip().endswith(\"'''\"):\n                                break\n                    break\n\n                break\n\n            if not has_comment:\n                functions.append((node.name, function_start))\n\n    return functions\n\ndef read_python_file(file_name):  \n    \"\"\"Read the content of a Python file.\"\"\"  \n    with open(file_name, 'r') as file:  \n        return file.read()  \n  \ndef extract_function_content(file_name, fns):  \n    with open(file_name, \"r\") as f:  \n        tree = ast.parse(f.read())  \n      \n    functions = []  \n    for node in ast.walk(tree):  \n        if isinstance(node, ast.FunctionDef):  \n            functions.append(node)  \n      \n    file_content = read_python_file(file_name)  \n    lines = file_content.splitlines()  \n      \n    extracted_functions = {}  \n    for node in functions:  \n        function_name = node.name  \n        function_start = node.lineno - 1  \n        function_end = node.body[-1].lineno  \n          \n        function_lines = lines[function_start:function_end]  \n        extracted_functions[function_name] = \"\\n\".join(function_lines)  \n\n    keys_to_keep = {t[0] for t in fns}  \n    filtered_fns = dict((k, v) for k, v in extracted_functions.items() if k in keys_to_keep) \n    return filtered_fns\n\n\ndef generate_comments_for_functions(file_name, functions):\n    \"\"\"Generate comments for functions by querying GPT.\"\"\"\n    new_comments = {}\n    extracted_fns = extract_function_content(file_name, functions)\n    \n    for function_name, function_content in extracted_fns.items():\n        # Prepare the prompt with function name and content\n        comment_prompt = (\n            f\"Generate a descriptive but concise single-line comment for the following function. The following function is written in Python, so please precede the comment with the character \\\"#\\\". Do not add any other notation, just the comment with the hashtag symbol:\\n\"\n            f\"Function Name: {function_name}\\n\\n\"\n            f\"Function Content:\\n{function_content}\\n\"\n        )\n\n        # Query GPT for the comment\n        \n        comment = query_model(prompt=comment_prompt)\n        comment = format_comment(comment)\n    \n        # Ensure the comment starts with a \"#\" or triple quotes and properly formatted\n        stripped_comment = comment.strip()\n        '''\n        if not (stripped_comment.startswith(\"#\") or stripped_comment.startswith('\"\"\"')):\n            comment = \"#\" + comment\n        elif stripped_comment.startswith('\"\"\"') and not stripped_comment.endswith('\"\"\"'):\n            comment = stripped_comment + ' \"\"\"'\n        elif stripped_comment.startswith(\"#\") and not stripped_comment.startswith(\"# \"):\n            comment = \"# \" + stripped_comment[1:]\n        '''\n        new_comments[function_name] = stripped_comment\n\n    return new_comments\n  \ndef format_comment(comment):  \n    \"\"\"Ensure the comment is properly formatted for Python files.\"\"\"  \n    if comment.startswith('#'):  \n        return comment  \n    if comment.startswith('\"\"\"') or comment.startswith(\"'''\"):  \n        return comment  \n    # If it's a multi-line comment, use triple quotes  \n    if '\\n' in comment:  \n        return f'\"\"\"\\n{comment}\\n\"\"\"'  \n    # Otherwise, use a single line comment  \n    return f'# {comment}'  \n  \nimport re  \n  \ndef insert_comments_into_file(input_file_path, output_file_path, comments):  \n    \"\"\"Insert comments before the corresponding function definitions and write to a new file.\"\"\"  \n    with open(input_file_path, 'r', encoding='utf-8') as file:  \n        lines = file.readlines()  \n  \n    # Create a pattern to match function definitions  \n    function_pattern = re.compile(r\"^def\\s+(\\w+)\\s*\\(\")  \n  \n    # We will iterate over the lines in reverse to avoid affecting the line numbers of subsequent functions  \n    for i in range(len(lines) - 1, -1, -1):  \n        match = function_pattern.match(lines[i].strip())  \n        if match:  \n            function_name = match.group(1)  \n            if function_name in comments:  \n                comment_lines = comments[function_name].split('\\n')  \n                lines.insert(i, '\\n'.join(comment_lines) + '\\n')  \n  \n    # Write the modified content to the output file  \n    with open(output_file_path, 'w', encoding='utf-8') as file:  \n        file.writelines(lines)\n\ndef comment_file(input_filename, output_filename):\n    functions = find_functions_with_no_comments(input_filename)\n    new_comments = generate_comments_for_functions(input_filename, functions) \n    insert_comments_into_file(input_filename, output_filename, new_comments) \n\nif __name__ == \"__main__\":\n    comment_file(\"src/ex_code.py\", \"src/ex_code_commented.py\")  # Replace with your input Python file\n\n    \n"
}